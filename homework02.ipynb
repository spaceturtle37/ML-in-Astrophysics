{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "###  Fitting a Line using a Maximum Likelihood Estimator\n",
    "\n",
    "Last week, you implcitly fitted straight lines with methods of moments estimators (i.e. sample mean and variance) and L-estimators (median and IQR). Generally though, we want some kind of uncertainty estimate for our models, and therefore M-estimators and maximum likelihood estimators in particular are useful.\n",
    "\n",
    "Assume the scatter in our measurements (the residuals) is generated by a gaussian process. I.e.:\n",
    "\n",
    ">$ y_i = a x_i + b + r_i $\n",
    "\n",
    "where $r_i$ is drawn from $N(0, \\sigma)$. Here, $\\sigma$ is the error the measurement induces.\n",
    "\n",
    "To use an M-estimator/MLE, you have to specify the likelihood function. First, the probability $p(y_i|x_i, M(a, b), \\sigma)$ that a particular point $y_i$ would be measured is just the normal distribution:\n",
    "\n",
    ">$ p(y_i|x_i, M(a, b), \\sigma) = N(y_i - M(x)|\\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( - \\frac{(y_i - M(x_i))^2}{2 \\sigma^2} \\right) $.\n",
    "\n",
    "Given what we discussed in class, we can write down the $\\ln L$\n",
    "\n",
    ">$ \\ln L(a, b) = constant - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^N (y_i - M(x_i))^2 $\n",
    "\n",
    "This is the expression that we now minimize with respect to $a$ and $b$ to find ML estimators for those parameters. \n",
    "\n",
    "\n",
    "And as we discussed in class, this is equivalent to minimizing the sum of the squares or a _least-squares method_.\n",
    "\n",
    "## MLE with outliers\n",
    "\n",
    "Let's apply the MLE to data with uncertainties where these uncertainties include outliers. \n",
    "I've defined a dataset below:\n",
    "\n",
    "Your mission is to:\n",
    "\n",
    "- Fit a line to the full sample by evaluating the squared loss likelihood (see the squared_loss function) on a grid of a, b.\n",
    "- Using the residuals from this best fit line, use sigma-clipping or an L-estimator to reject outliers with large residuals. Fit a line to the data with outliers rejected.\n",
    "- Define a new likelihood function that implements the Huber loss (https://en.wikipedia.org/wiki/Huber_loss), also incorporating the measurement uncertainties $dy$. (This step is already done for you: See the function `huber_loss`.)\n",
    "- Fit a new line to all of the data (no outlier rejection) with the new Huber likelihood, except this time use scipy.optimize.fmin instead of a grid search.\n",
    "\n",
    "Recommended reading: David Hogg, Jo Bovy, and Dustin Lang: \"Data analysis recipes: Fitting a model to data\", 2010: https://arxiv.org/abs/1008.4686"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a straight line\n",
    "def line(a, b, x):\n",
    "    return a*x + b\n",
    "\n",
    "def zres(a, b, x, y, dy):\n",
    "    z = (y - line(a, b, x))/dy\n",
    "    return z\n",
    "    \n",
    "# Define a standard squared-loss function\n",
    "def squared_loss(a, b, x, y, dy):\n",
    "    # we'll add the extra 1/2 just to make the scaling the same as the Huber loss - just a constant\n",
    "    z = zres(a, b, x, y, dy)\n",
    "    return np.sum(z**2.)/2. \n",
    "\n",
    "# Define the Huber loss function\n",
    "def huber_loss(a, b, x, y, dy, delta=1.):\n",
    "    assert delta > 0., \"keep your deltas positive\"\n",
    "    z = zres(a, b, x, y, dy)\n",
    "    zabs = np.abs(z)\n",
    "    loss = zabs*0.\n",
    "    ind_out = (zabs > delta)\n",
    "    loss[ind_out] = delta*zabs[ind_out] - delta**2./2.\n",
    "    loss[~ind_out] = z[~ind_out]**2./2\n",
    "    return np.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.optimize\n",
    "from astroML.datasets import fetch_hogg2010test\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Get data: this includes outliers\n",
    "data = fetch_hogg2010test()\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "dy = data['sigma_y']\n",
    "\n",
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 2: Optimal Photometry\n",
    "\n",
    "We now turn to examples of applications of ML in astronomy. \n",
    "\n",
    "We'll look at two cases: simple photometry and simultaneous fitting of galaxy size and total flux.\n",
    "\n",
    "Last week, we had you download a bunch of SDSS data files and plot `MAG_PSF` from them. But what is this quantity and how is it calculated?\n",
    "\n",
    "Have a look at one of the example images we saw as an example of astronomical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### OPTIONAL: RUN THIS CELL \n",
    "###\n",
    "### This will open a FITS data file in the software DS9, which specializes in viewing FITS images.\n",
    "### You will need to have DS9 installed ()\n",
    "###\n",
    "### Note - Jupyter may not be able to open a program like DS9, depending on your setup. If this cell\n",
    "### does not work for you, try running the \"ds9\" command in your computer's terminal.\n",
    "\n",
    "! ds9 -scale zscale data/wdd7.071117_0328.073_6.sw.fits\n",
    "\n",
    "# You can compare the simulated soures below to the real sources in this image. They should look quite similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A star and a point spread function\n",
    "\n",
    "The stars in those images are not point sources, despite being trillions of kilometers from us. This is because as the light from the stars meet travel to meet their fate on our CCD detectors here on Earth, it gets spread out from a perfect point.\n",
    "\n",
    "The **Point Spread Function** (the \"PSF\") is a function that descibes how a point source of light is dispersed over a telescope's detector. It's composed of various contributions: from minute misalignments in the optics, to jitter in the tracking, to diffraction from the spider, to diffusion of the electrons as they travel through the silicon in the CCD (\"charge diffusion\"), to the homogenization of the PSF due to the refraction and diffraction in the atmosphere.  \n",
    "\n",
    "The dominant effect is the one ***from the atmosphere***.\n",
    "\n",
    "![PSF Formation](figures/psf-formation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we don't construct the PSF from *ab initio* considerations because it's impractical to track each of these effects separately.\n",
    "\n",
    "Instead, we measure it by looking at the observed shape of bright, isolated, point sources (i.e., stars). Those observations are noisy, so we fit analytical profiles to estimate the true PSF.\n",
    "\n",
    "For simplicity, we will use just a single bivariate Gaussian here.\n",
    "\n",
    "This is our model.\n",
    "\n",
    "\n",
    "\n",
    "As before, we'll generate some data - but not for 1000s of sources. Just one. It'll be stored in a variable named `image`\n",
    "You can see the true parameters of the image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT ALTER THIS CODE:\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from astroML.plotting.mcmc import convert_to_stdev\n",
    "np.random.seed(seed=42)\n",
    "\n",
    "def plotCutout(img, title = None, clabel='$counts$', xlabel='x (pixels)', ylabel='y (pixels)', loc=None, fig=None):\n",
    "    \"\"\"Make a nice looking plot of a small image\"\"\"\n",
    "    if loc is None:\n",
    "        fig, ax = plt.subplots(figsize=(8,8))\n",
    "        fig.subplots_adjust(right=0.8, bottom=0.34)\n",
    "    else:\n",
    "        ax = fig.add_subplot(loc)\n",
    "\n",
    "    # coordinate of the edge (assuming a square image)\n",
    "    xe = img.shape[0] / 2.\n",
    "\n",
    "    if title is None:\n",
    "        title = \"min = %.0f, max=%.0f\" % (img.min(), img.max())\n",
    "    \n",
    "    ax.set_title(title, fontsize=14)\n",
    "    plt.imshow(img, origin='lower', interpolation='nearest',\n",
    "           extent=(-xe, xe, -xe, xe),\n",
    "           cmap=plt.cm.binary, aspect=1)\n",
    "    plt.clim(-20, 100)\n",
    "    plt.colorbar().set_label(clabel)\n",
    "\n",
    "    ax.set_xlabel(xlabel, fontsize=12)\n",
    "    ax.set_ylabel(ylabel, fontsize=12)\n",
    "\n",
    "\n",
    "# These will be the parameters of our image:\n",
    "\n",
    "Atrue = 10000.0    # the source count normalization\n",
    "xdim = 15         # width of the image\n",
    "ydim = 15         # height of the image\n",
    "muXtrue = 0.0     # source x centroid \n",
    "muYtrue = 0.0     # source y centroid \n",
    "sigmaPSF = 1.5    # in pixels, corresponds to seeing = 1.5*0.2*2.355 = 0.7 arcsec for LSST (0.2\" pix)\n",
    "skyBg = 25        # sky background\n",
    "\n",
    "\n",
    "def makePSF(shape, x0, y0, sigmaPSF):\n",
    "    # Add a (Gaussian) PSF of width sigmaPSF, centered on (x0, y0)\n",
    "    xx = np.arange(shape[0]) - shape[0]/2.+0.5\n",
    "    yy = np.arange(shape[1]) - shape[1]/2.+0.5\n",
    "    \n",
    "    r = np.sqrt((xx[:, None]-x0)**2 + (yy[None, :]-y0)**2)\n",
    "    psf = np.exp(-r**2./2./(sigmaPSF**2.)) / (np.sqrt(2.*math.pi)*sigmaPSF)\n",
    "    return psf\n",
    "\n",
    "fig = plt.figure(figsize=(12,18))\n",
    "\n",
    "# uniform flux\n",
    "image = np.zeros((xdim, ydim))\n",
    "image[7,7] = Atrue\n",
    "plotCutout(image, loc=321, fig=fig, title=\"Truth\")\n",
    "\n",
    "# random Poisson process\n",
    "image = np.zeros((xdim, ydim))\n",
    "image[7,7] = np.random.poisson(Atrue)\n",
    "plotCutout(image, loc=322, fig=fig, title=\"Perfect detector, no atmosphere\")\n",
    "\n",
    "# PSF \n",
    "image = np.zeros((xdim, ydim))\n",
    "image = Atrue * makePSF(image.shape, 0, 0, sigmaPSF)\n",
    "plotCutout(image, loc=323, fig=fig, title=\"Now with atmosphere\")\n",
    "\n",
    "# PSF with background\n",
    "image += skyBg\n",
    "plotCutout(image, loc=324, fig=fig, title=\"Now with atmosphere and background\")\n",
    "\n",
    "# PSF with background and noise\n",
    "image = np.random.poisson(image)\n",
    "plotCutout(image, loc=325, fig=fig, title=\"W/ Atmosphere,background and noise\")\n",
    "\n",
    "simulatedImage = image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Photometry (measuring the flux)\n",
    "\n",
    "We now have an image that we observe, and some ***understanding of the processes that created it***.\n",
    "We have encoded that understanding into a model.\n",
    "This means you can now use maximum likelihood to estimate the parameters of the underlying source - like the total area under the profile - a measurement of how much light we received from the source, or it's `FLUX`\n",
    "\n",
    "Reminder: The `MAG_PSF` you plotted from the SDSS HLC files last week was a magnitude measurement, so it was directly related to the flux by $-2.5*log_{10}(\\text{Flux}) + \\text{Constant}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PSF Photometry: A Maximum Likelihood estimation of the flux\n",
    "\n",
    "Our **model** for the image is that it consists of\n",
    "- a Gaussian with a flux normalization of $A$ (aka: a total flux summing to $A$),\n",
    "- plus the background (which we've removed),\n",
    "- plus the Gaussian noise (caused by photon quantization).\n",
    "\n",
    "So, <u>**once the background is removed**</u>, for ***each pixel*** I can write:\n",
    "\n",
    ">$ I(x,y) = A * PSF(x - x_0, y - y_0) + r_i $\n",
    "\n",
    "$A$ is the flux of the star, which is what we want to estimate.\n",
    "\n",
    "$r_i$ is the noise, which is drawn from a Gaussian with mean 0 and std. deviation\n",
    "\n",
    ">$\\sigma_{x,y} = \\sqrt{I_{x,y} + B}$\n",
    "\n",
    "and $B$ is the sky background. It's critical not to forget the background when computing the sigma -- for faint sources, this is where most of the measurement uncertainty comes from!\n",
    "\n",
    "### For a single pixel \n",
    "\n",
    "### $$\n",
    "\\begin{align}\n",
    "p(I(x_i, y_i)|A, PSF(x_i-x_0, y_i-y_0), \\sigma) &= N(I(x_i, y_i) - A*PSF(x_0, y_0)|\\sigma) \\\\\n",
    "& = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} A\\exp \\left( - \\frac{(I(x_i, y_i) - A*PSF(x_0, y_0))^2}{2 \\sigma^2} \\right) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### For a whole image then $p1 \\times p2 \\times p3$....\n",
    "\n",
    "### $$ \n",
    "\\begin{align}\n",
    "P &= \\Gamma p(I(x, y)|A, PSF(x_i-x_0, y_i-y_0), \\sigma) \\\\\n",
    "& = N(I(x_i, y_i) - A*PSF(x_0, y_0)|\\sigma) \\\\\n",
    "& = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} A\\exp \\left( - \\frac{(I(x_i, y_i) - A*PSF(x_0, y_0))^2}{2 \\sigma^2} \\right) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note this is practically identical to our case with fitting a line, except that instead of having one index $i$, we have two indices for the data points, $(x, y)$. And ***each pixel*** is an independent data point that constrains our model.\n",
    "\n",
    "So we can readily write out the log-likelihood as:\n",
    "\n",
    "### $$\\ln{L} = \\text{constant} - \\sum_{i=1}^N \\frac{\\left(I(x_i,y_i) - A\\  \\mathrm{PSF}(x_0,y_0)\\right)^2}{2 \\sigma^2}$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "## $$ \\ln L(A) = C + \\sum_{x,y} \\frac{I_{x,y} A \\cdot PSF(x - x_0, y - y_0)}{\\sigma_{x,y}^2} - \\sum_{x,y} \\frac{A^2 \\cdot PSF(x - x_0, y - y_0)^2}{2 \\sigma_{x,y}^2}$$\n",
    "\n",
    "where the $I^2$ term can just be absorbed into to the constant.\n",
    "\n",
    "If we set \n",
    "## $$\\phi_{x,y} = \\sum_{x,y} \\frac{I_{x,y} \\cdot PSF_{x,y}}{\\sigma_{x,y}^2}$$\n",
    "\n",
    "and \n",
    "## $$\\psi_{x,y} = \\sum_{x,y} \\frac{PSF_{x,y}^2}{\\sigma_{x,y}^2}$$\n",
    "\n",
    "then\n",
    "\n",
    "## $$ \\ln L(A) = C + A\\cdot\\phi_{x,y} - A^2\\cdot \\frac{\\psi_{x,y}}{2} $$\n",
    "\n",
    "As we've seen , maximizing the likelihood of a product of normal distribution reduces to minimizing the $\\chi^2$, which is what you'll do next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.1 Assume the noise is constant, and minimize the log-likelihood analytically. \n",
    "I.e: For what value of $A$ is $\\ln L(A)$ minimized? (Some calculus required)\n",
    "\n",
    "In addition, look at the form of $\\phi_{x,y}$ and consider it for each pixel. **What does it represent?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR ANSWER HERE\n",
    "\n",
    "You can format equations in markdown - there's plenty of examples in this notebook itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Q2.2 Estimating the true flux\n",
    "\n",
    "To estimate the true flux, we need to maximize the likelihood. That is equivalent to minimizing the $\\chi^2$. As a reminder, our basic model is:\n",
    "\n",
    ">$ I(x,y) = A * PSF(x - x_0, y - y_0) + r_i $\n",
    "\n",
    "* estimate the background in the image\n",
    "* estimate the noise in our measurements (remember it's a Poisson process!)\n",
    "* as with the straight line estimate the true flux grid of A\n",
    "* Destermine the chi-sq/DoF at each location on the A grid and plot it\n",
    "* Compare the numerical estimate with your analytical estimate that you made with your answer to Q2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Models with more parameters\n",
    "\n",
    "The iterative example above is a bit contrived as we can, and did, solve the MLE analytically \n",
    "\n",
    "It has the benefit of being general, however. We can apply the same technique to models with more parameters (within the limits of computing power available to us!). The other parameters could be the position of the object ($x$, $y$), or some measure of the shape of the object.\n",
    "\n",
    "As the number of parameters increases, the likelihood curve from the previous slide becomes the ***likelihood (hyper)surface***. The more dimensions there are, the more difficult (computationally expensive) it becomes to find its maximum using brute force solutions; even for 2D cases, we're likely to resort to ***minimization algorithms*** (e.g., Levenberg–Marquardt or others)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit: Homework Survey\n",
    "https://forms.gle/75i4q3DBgi2pChY1A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did you fill it out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extra Credit: Measuring the size and brightness of a spherical, gaussian, galaxy\n",
    "\n",
    "In Homework 1, you had `MAG_PSF` not just for stars, but also for galaxies. \n",
    "\n",
    "Even in the ideal (space) case, the Galaxy will not be a point source. We'll imagine we're dealing with a \"Gaussian spherical galaxy\" -- i.e., one whose intensity  falls of as a 2D gaussian would. As with the star, we'll asume the position of the galaxy is known.\n",
    "\n",
    "Our model has two parameters: the measure of the extendedness of the galaxy -- $\\sigma$ -- and its total flux, $C$:\n",
    "\n",
    ">$ I(x, y) = \\frac{C}{2 \\pi \\sigma^2} \\exp \\left( -\\frac{(x-x_0)^2 + (y-y_0)^2}{2\\sigma^2} \\right) $\n",
    "\n",
    "We will now have to explore the likelihood surface in $(C, \\sigma)$ space, and find the point of its maximum.\n",
    "\n",
    "Note #2: When generating the simulated image, $I(x, y)$ needs to be ***convolved*** with the PSF. Fortunately (actually, because I'm lazy) we've chosen the galaxy profile to be gaussian, and the convolution of two gaussians with variances $\\sigma_1^2$ and $\\sigma_2^2$ is also a gaussian with the variance $\\sigma_1^2 + \\sigma_2^2$ (i.e., it's wider). \n",
    "\n",
    "Our function `gauss2D` utilizes this fact to generate the convolved image of the galaxy.\n",
    "\n",
    "* we will generate two cases with `gauss2D` below:\n",
    "* using this image as your data evaluate the log-likelihood\n",
    "    * in Q2, we only did this for the flux $A$\n",
    "    * now you have two parameters $C$ and it's shape $\\sigma$\n",
    "    * evaluate the likelihood on this grid \n",
    "    \n",
    "    `C = np.linspace(500, 1500, 101)`\n",
    "    \n",
    "    `Sigma = np.linspace(0, 2.5, 101)`\n",
    "    \n",
    "* plot your estimated log-likehood surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss2D(muX, muY, sig, A, skyBg, sigPSF, Xpixels, Ypixels):\n",
    "    \"\"\"\n",
    "    Generate an image of size (Xpixels, Ypixels) with\n",
    "    a 2D circular Gaussian of total flux A\n",
    "    with standard deviation sigma^2=sig^2 + sigPSF^2\n",
    "    superimposed on a uniform background B and centered\n",
    "    on (muX, muY).\n",
    "    \"\"\"\n",
    "    r = np.sqrt((Xpixels-muX)**2 + (Ypixels-muY)**2)\n",
    "    # make and set image to the background value\n",
    "    image = np.empty(r.shape)\n",
    "    image.fill(skyBg)\n",
    "\n",
    "    ## now add circular gaussian profile (normalized to A)\n",
    "    # source gauss convolved with single-gauss PSF  \n",
    "    sigConvSquare = sig**2 + sigPSF**2\n",
    "    image += A*np.exp(-r**2/2/sigConvSquare) / (2*math.pi*sigConvSquare)\n",
    "    return image\n",
    "\n",
    "\n",
    "def addnoise(inimage, sigNoise, sourceImage, addsourcenoise=0): \n",
    "    \"\"\"Add gaussian noise to the image, and return the image and variance plane\"\"\"\n",
    "    image = np.copy(inimage)\n",
    "    image += np.random.normal(0, sigNoise, image.shape)\n",
    "    variance = 0*image + sigNoise**2\n",
    "\n",
    "    if (addsourcenoise):\n",
    "        gain = 1.0  # as a reminder...\n",
    "        sourceVariance = sourceImage/gain\n",
    "        image += np.random.normal(0, np.sqrt(sourceVariance), image.shape)\n",
    "        variance += sourceVariance\n",
    "\n",
    "    return image, variance \n",
    "\n",
    "\n",
    "def simulate(muXtrue, muYtrue, sigtrue, Atrue, skyBg, sigmaPSF, sigmaNoise):\n",
    "    # set seed\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # define the (square) grid\n",
    "    xpix = np.linspace(-7, 7, 15)\n",
    "\n",
    "    ## make psf (sigtrue=0) \n",
    "    psf = gauss2D(muXtrue, muYtrue, 0, Atrue, skyBg, sigmaPSF, xpix[:, np.newaxis], xpix)\n",
    "\n",
    "    ## make noiseless image (convolved with psf given by sigmaPSF, image size given by 1Dpixels) \n",
    "    nonoise = gauss2D(muXtrue, muYtrue, sigtrue, Atrue, skyBg, sigmaPSF, xpix[:, np.newaxis], xpix)\n",
    " \n",
    "    ## now add noise\n",
    "    image, variance = addnoise(nonoise, sigmaNoise, 0) \n",
    "\n",
    "    ## difference object - psf\n",
    "    diffimage = image - psf\n",
    "\n",
    "    return nonoise, psf, image, diffimage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Case 1 - a sanity check - if sigtrue (i.e. the shape of the galaxy) is 0 we're back to dealing with a single star."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "muXtrue = 0.0     # source x centroid \n",
    "muYtrue = 0.0     # source y centroid \n",
    "sigtrue = 0.0     # the intrinsic gaussian source width, sigma (in pixels) \n",
    "Atrue = 1000.0    # the source count normalization\n",
    "skyBg = 25        # sky background\n",
    "sigmaPSF = 1.5    # in pixels, corresponds to seeing = 1.5*0.2*2.355 = 0.7 arcsec for LSST (0.2\" pix)\n",
    "sigmaNoise = 5.0  # gaussian sigma for white noise (counts), e.g. due to sky \n",
    "\n",
    "\n",
    "nonoise, psf, image, diffimage = simulate(muXtrue, muYtrue, sigtrue, Atrue, skyBg, sigmaPSF, sigmaNoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE TO SOLVE THE HOMEWORK GOES HERE - TRY TO MAKE A FUNCTION YOU CAN REUSE FOR CASE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Case 2 - if sigtrue is non-zero, we're dealing with a spherical cow galaxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "muXtrue = 0.0     # source x centroid \n",
    "muYtrue = 0.0     # source y centroid \n",
    "sigtrue = 1.0     # the intrinsic gaussian source width, sigma (in pixels) \n",
    "Atrue = 1000.0    # the source count normalization\n",
    "skyBg = 25        # sky background\n",
    "sigmaPSF = 1.5    # in pixels, corresponds to seeing = 1.5*0.2*2.355 = 0.7 arcsec for LSST (0.2\" pix)\n",
    "sigmaNoise = 5.0  # gaussian sigma for white noise (counts), e.g. due to sky \n",
    "\n",
    "nonoise, psf, image, diffimage = simulate(muXtrue, muYtrue, sigtrue, Atrue, skyBg, sigmaPSF, sigmaNoise)\n",
    "\n",
    "# IF YOU WROTE A SENSIBLE FUNCTION FOR THE STAR CASE, YOU CAN JUST REUSE IT HERE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every one of the measurements in the SDSS HLC files was estimated using a more sophisticated version of this process."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
