{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: How to implement a neural network - gradient descent\n",
    "\n",
    "In this problem, we will introduce how to implement a very simple neural network from scratch with Python. Then we will optimize it by hand using the gradient-descent technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import numpy as np  # Matrix and vector computation package\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt  # Plotting library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The neural network\n",
    "\n",
    "In this problem we will go into the details of [gradient descent](http://en.wikipedia.org/wiki/Gradient_descent) illustrated on a very simple network: a 1-input 1-output [linear regression](https://en.wikipedia.org/wiki/Linear_regression) model that has the goal to predict the target value $t$ from the input value $x$.\n",
    "\n",
    "**The network** is defined as having an input $\\mathbf{x}$ which gets transformed by the weight $\\mathbf{w}$ to generate the output $\\mathbf{y}$ by the formula $\\mathbf{y} = \\mathbf{w} \\cdot \\mathbf{x}$. This network can be represented graphically as:\n",
    "\n",
    "![Image of the simple neural network](figures/SimpleANN01.png)\n",
    "\n",
    "We'll use the following setup to give this model both a slope and an intercept:\n",
    "\n",
    "- We'll use $\\mathbf{x_i} = \\{1, x_i\\}$ to represent each input value $x$.\n",
    "- We'll use $\\mathbf{w} = \\{w_0, w_1\\}$ to represent the network's weights.\n",
    "\n",
    "Then, the network is equivalent to the linear model $y_i = w_0 + w_1 x_i$.\n",
    "\n",
    "**Our goal** is to find the value of $\\mathbf{w}$ such that $\\mathbf{y}$ approximates the targets $\\mathbf{t}$ as well as possible.\n",
    "\n",
    "In practice, neural networks typically have **many** nodes like this over many different layers, and each node has a non-linear activation function. This simple network only has an input node, an output node, and a linear activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the loss function\n",
    "\n",
    "We will optimize the model $\\mathbf{y} = \\mathbf{x} * \\mathbf{w}$ by tuning parameter $\\mathbf{w}$ so that the [mean squared error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error) along all samples is minimized.\n",
    "\n",
    "The **mean squared error** is defined as\n",
    "\n",
    "$$\n",
    "\\xi = \\frac{1}{N} \\sum_{i=1}^{N} \\Vert t_i - y_i \\Vert ^2\n",
    "$$\n",
    "with $N$ the number of samples in the training set.\n",
    "\n",
    "Notice that we take the mean of errors over all samples, which is known as **batch training**. We could also update the parameters based upon one sample at a time, which is known as **online training**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing the loss function: Gradient descent\n",
    "\n",
    "One optimization algorithm commonly used to train neural networks is the [gradient descent](http://en.wikipedia.org/wiki/Gradient_descent) algorithm.\n",
    "\n",
    "Gradient descent is based on the idea that, as a parameter $w$ gets farther from its optimal value, the **gradient the of loss $\\xi$ with respect to $w$** (ie. $\\partial \\xi / \\partial w$) gets larger. Therefore, it updates the position of $\\mathbf{w}$ proportionally to $- \\partial \\xi / \\partial w$ to reduce the loss. This is illustrated in the figure below for a single parameter $w$:\n",
    "\n",
    "<img src=\"figures/Loss_gradient_descent_small.png\" alt=\"Gradient descent example\" style=\"width: 450px;\"/>\n",
    "\n",
    "The parameter $w$ is updated iteratively throughout the gradient descent:\n",
    "\n",
    "$$\n",
    "w(k+1) = w(k) - \\Delta w(k)\n",
    "$$\n",
    "\n",
    "where $w(k)$ is the value of $w$ at iteration $k$ during the gradient descent, and $\\Delta w$ is proportional to the gradient:\n",
    "\n",
    "$$\n",
    "\\Delta w = \\mu \\frac{\\partial \\xi}{\\partial w}\n",
    "$$\n",
    "\n",
    "$\\mu$ is called the **learning rate**, which is how big a step you take along the gradient.\n",
    "\n",
    "**How do we calculate the gradient ${\\partial \\xi}/{\\partial w}$?** We can use the definitions of $y$ and $\\xi$ to develop a simple expression for it.\n",
    "\n",
    "For each sample $i$ this gradient can be split according to the [chain rule](http://en.wikipedia.org/wiki/Chain_rule) into:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\xi_i}{\\partial w} = \\frac{\\partial \\xi_i}{\\partial y_i} \\frac{\\partial y_i}{\\partial w}\n",
    "$$\n",
    "\n",
    "Where $\\xi_i$ is the squared error loss, so the ${\\partial \\xi_i}/{\\partial y_i}$ term can be written as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\xi_i}{\\partial y_i} = \\frac{\\partial (t_i - y_i)^2}{\\partial y_i} = - 2 (t_i - y_i) = 2 (y_i - t_i)\n",
    "$$\n",
    "\n",
    "And since $y_i = x_i \\cdot w$ we can write ${\\partial y_i}/{\\partial w}$ as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial w} = \\frac{\\partial (x_i \\cdot w)}{\\partial w} = x_i\n",
    "$$\n",
    "\n",
    "So the full update function $\\Delta w$ for sample $i$ will become:\n",
    "\n",
    "$$\n",
    "\\Delta w = \\mu \\cdot \\frac{\\partial \\xi_i}{\\partial w} = \\mu \\cdot 2 x_i (y_i - t_i)\n",
    "$$\n",
    "\n",
    "The above equation calculates the gradient based on the squared error of one sample $\\mathbf{x_i}$. In \"batch processing\", we take the gradient on the mean squared error of all samples:\n",
    "\n",
    "$$\n",
    "\\boxed{\\Delta w = \\mu \\cdot 2 \\cdot \\frac{1}{N} \\sum_{i=1}^{N} x_i (y_i - t_i)}\n",
    "$$\n",
    "\n",
    "The gradient descent algorithm is typically initialised by starting with random initial parameters $\\mathbf{w}$. After initiating these parameters we can start updating with $\\Delta w$ until convergence. The learning rate $\\mu$ needs to be tuned separately as a hyperparameter for each neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Your task:\n",
    "\n",
    "Your job is to implement the neural network, the loss function, and the gradient function. Then, you will use these to implement the gradient descent algorithm described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below,\n",
    "- The neural network model should be implemented in the `nn(x, w)` function\n",
    "- The loss function should be implemented in the `loss(y, t)` function\n",
    "- The gradient ${\\partial \\xi}/{\\partial w}$ should be implemented int the `gradient(x, y, t)` function\n",
    "\n",
    "Implement these using the definitions above. (Note: The function contracts at the beginning are quite long for your benefit, but they shouldn't require much code!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn(x,w):\n",
    "    \"\"\"Implements the neural network. Estimates y from x and w\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy array, shape(N, 2)\n",
    "        The neural network input. Should take the form [[1, x_0], [1, x_1], ... [1, x_n]]\n",
    "    w : numpy array, shape(2,)\n",
    "        The weights. Should take the form [w_0, w_1]\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    numpy array, shape(N,)\n",
    "        The estimated value of y\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "def loss(y,t):\n",
    "    \"\"\"Calculates the mean squared error loss on y relative to target values t\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : numpy array, shape(N,)\n",
    "        The array of estimated values (output by the neural network)\n",
    "    t : numpy array, shape(N,)\n",
    "        The array of target (true) values\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The mean squared error loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "def gradient(x,y,t):\n",
    "    \"\"\" Calculates the gradient ( d loss / d w_i ) for the given input, output,\n",
    "    and target values of the neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy array, shape(N, 2)\n",
    "        The neural network input. Should take the form [[1, x_0], [1, x_1], ... [1, x_n]]\n",
    "    y : numpy array, shape(N,)\n",
    "        The neural network output\n",
    "    t : numpy array, shape(N,)\n",
    "        The array of target (true) values\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    numpy array, shape(2,)    \n",
    "        The gradient of mean squared error loss with respect to the parameters w\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(x_train, t, num_epochs=1000, learn_rate=1E-2):\n",
    "    \"\"\"Trains the neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_train : numpy array, shape(N,)\n",
    "        The input data on which to train\n",
    "    t_train : numpy array, shape(N,)\n",
    "        The target data on which to train\n",
    "    num_epochs : int, optional\n",
    "        Total number of epochs to train for. (Default 100)\n",
    "    learn_rate : float, optional\n",
    "        The learning rate mu. (Default 0.01)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    weights : numpy array, shape(num_epochs,)\n",
    "        The weight vector w used at each epoch\n",
    "    wlosses : numpy array, shape(num_epochs,)\n",
    "    \n",
    "    \"\"\"\n",
    "    N = x_train.shape[0]\n",
    "    \n",
    "    # Appends a column of ones to the left of x to form the neural network input array\n",
    "    x = np.hstack((np.ones(N).reshape(N, 1), x_train))\n",
    "    \n",
    "    # Initializes the weight vector\n",
    "    w = np.array([0.0,0.0])\n",
    "    \n",
    "    weights = []\n",
    "    wlosses = []\n",
    "\n",
    "    for k in range(num_epochs):\n",
    "        # Train the network by calculating y, calculating the gradient,\n",
    "        # and updating the parameters `w`\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        y = ...\n",
    "        grad ...\n",
    "        w =  ...\n",
    "        \n",
    "        # Add the weights and loss to a list for each epoch\n",
    "        weights.append(w.copy())\n",
    "        wlosses.append(loss(y,t))\n",
    "            \n",
    "    return weights,wlosses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: Train your network\n",
    "\n",
    "In the cells below, we revisit the familiar Cepheid variable star dataset (magnitude vs. oscillation period). If your training function works, you should be able to recover a very good best-fit line through this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the Cepheid dataset\n",
    "\n",
    "exec(open('cepheids.py').read())\n",
    "ceph = Cepheids('data/R11ceph.dat')\n",
    "hosts = ceph.list_hosts()\n",
    "\n",
    "ID = 5\n",
    "host = hosts[ID]\n",
    "ceph.select(host)\n",
    "mobs = ceph.mobs\n",
    "logP = ceph.logP\n",
    "sigma_obs = ceph.sigma\n",
    "\n",
    "# Separates the data into training and testing\n",
    "\n",
    "train_idx = np.random.choice(np.arange(len(mobs)),int(2*len(mobs)/3),replace=False)\n",
    "test_idx = np.setdiff1d(np.arange(len(mobs)), train_idx)\n",
    "\n",
    "train_mobs = np.atleast_2d(ceph.mobs[train_idx])\n",
    "train_logP = np.atleast_2d(ceph.logP[train_idx]).T\n",
    "train_sigma_obs = np.atleast_2d(ceph.sigma[train_idx]).T\n",
    "\n",
    "test_mobs = np.atleast_2d(ceph.mobs[test_idx])\n",
    "test_logP = np.atleast_2d(ceph.logP[test_idx]).T\n",
    "test_sigma_obs = np.atleast_2d(ceph.sigma[test_idx]).T\n",
    "\n",
    "# Plots magnitude vs. log oscillation period\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.errorbar(train_logP.flatten(), train_mobs.flatten(), yerr=train_sigma_obs.flatten(), linestyle='None', marker='o', label='Training data')\n",
    "ax.errorbar(test_logP.flatten(), test_mobs.flatten(), yerr=test_sigma_obs.flatten(), linestyle='None', marker='o', label='Test data')\n",
    "ax.legend(frameon=False)\n",
    "ax.set_xlabel('log Period')\n",
    "ax.set_ylabel('Mag (corrected) + Offset')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with this to get the best result you can!\n",
    "learn_rate = 0.2 \n",
    "\n",
    "weights, wlosses = train(train_logP, train_mobs, num_epochs=500, learn_rate=learn_rate)\n",
    "\n",
    "w = weights[-1]\n",
    "y_test = [nn(np.array([1, logP]), w) for logP in test_logP]\n",
    "\n",
    "train_loss = wlosses[-1]\n",
    "test_loss = loss(y_test, test_mobs)\n",
    "\n",
    "print(f\"Weights: w_0 (intercept) = {w[0]:.2f} and w_1 (slope) = {w[1]:.2f}\")\n",
    "print(f\"Training loss (MSE): {wlosses[-1]:.2f}\")\n",
    "print(f\"Testing loss (MSE): {test_loss:.2f}\")\n",
    "\n",
    "# -----------------------------------\n",
    "\n",
    "# Plots the results of your training\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('ticks')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rc('xtick',labelsize=18)\n",
    "plt.rc('ytick',labelsize=18)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2,figsize=(13,5))\n",
    "\n",
    "numepochs = len(weights)\n",
    "halfepochs = int(np.floor(numepochs/2))\n",
    "\n",
    "x = np.linspace(np.min(train_logP),np.max(train_logP),10)\n",
    "y_final = weights[-1][1] * x + weights[-1][0]\n",
    "y_1e = weights[0][1] * x + weights[0][0]\n",
    "y_50e = weights[49][1] * x + weights[49][0]\n",
    "\n",
    "ax1.errorbar(logP, mobs, yerr=sigma_obs, linestyle='None', marker='o', label=ID)\n",
    "ax1.plot(x,y_final,linewidth=2,label=f'{numepochs} epochs: w=[{weights[-1][0]:.2f},{weights[-1][1]:.2f}]')\n",
    "\n",
    "ax1.legend(fancybox=True)\n",
    "\n",
    "ax2.semilogy(np.arange(len(wlosses)),wlosses)\n",
    "ax2.set_title('Loss Function',size=18)\n",
    "ax2.set_xlabel('Epochs',size=18)\n",
    "ax2.set_ylabel('MSE',size=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Binary classification with a random forest classifier\n",
    "\n",
    "In this class, we have seen the basics of how a random forest classifier works as well as how useful it can be. We will explore that further in this problem by applying it to a particularly messy dataset: The [Zwicky Transient Facility's (ZTF)](https://www.ztf.caltech.edu/) database of transient candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro: Astronomical transients and the \"real/bogus\" problem\n",
    "\n",
    "Astronomical transients are events that cause the brightness of an object to vary on scales that human beings can observe. Many, such as supernovae, vary on scales of hours to days. ZTF searches for new transients using a technique called **difference imaging**: It images large sections of the night sky periodically, then takes the difference of new images with a \"reference image\", which is the \"average\" sky seen in the past. Significant differences between the new image and the average could be a sign of a new astronimical transient.\n",
    "\n",
    "Unfortunately, difference imaging is sensitive to ALL types of differences -- including those caused by errors or glitches in the image processing pipeline. Distinguishing detections of \"real\" transients from their \"bogus\" counterparts is generally called the **\"real-bogus problem\"**.\n",
    "\n",
    "ZTF solves their real-bogus problem by using a deep convolutional neural network, which is trained directly on the difference images. (The paper describing this method is available here: https://academic.oup.com/mnras/article/489/3/3582/5554758). We will use a much simpler approach. In this problem, you will train a random forest classifier to distinguish \"real\" and \"bogus\" images based on a set of over 30 features extracted from ZTF difference images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem outline\n",
    "\n",
    "Part 1: Visualize the data and identify the features that best distinguish \"real\" images from \"bogus\" images.\n",
    "\n",
    "Part 2: Train a random forest classifier to identify real/bogus images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "In the cell below, we load data from 3,190 candidate transient images into the Pandas data frame `ZTF_sources_dataframe`. Their true labels are stored in the final column, `label`. Label 0 means \"bogus\" and label 1 means \"real\".\n",
    "- **Source:** `ZTF_sources_dataframe`'s columns come from the ZTF `alerts.candidates` table, [described here](https://zwickytransientfacility.github.io/ztf-avro-alert/schema.html) if you'd like to learn more about what they mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_META = 'data/dsfp_ztf_meta.npy'\n",
    "F_FEATS = 'data/dsfp_ztf_feats.npy'\n",
    "\n",
    "meta = np.load(F_META, allow_pickle=True)\n",
    "feats = np.load(F_FEATS, allow_pickle=True)\n",
    "\n",
    "COL_NAMES = ['diffmaglim', 'magpsf', 'sigmapsf', 'chipsf', 'magap', 'sigmagap',\n",
    "             'distnr', 'magnr', 'sigmagnr', 'chinr', 'sharpnr', 'sky',\n",
    "             'magdiff', 'fwhm', 'classtar', 'mindtoedge', 'magfromlim', 'seeratio',\n",
    "             'aimage', 'bimage', 'aimagerat', 'bimagerat', 'elong', 'nneg',\n",
    "             'nbad', 'ssdistnr', 'ssmagnr', 'sumrat', 'magapbig', 'sigmagapbig',\n",
    "             'ndethist', 'ncovhist', 'jdstarthist', 'jdendhist', 'scorr', 'label']\n",
    "\n",
    "ZTF_sources_dataframe = pd.DataFrame(columns=COL_NAMES, data=feats)\n",
    "\n",
    "ZTF_sources_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Visualize the data\n",
    "\n",
    "Before trying to classify the data, visualize it in the two ways described below. You should be able see that no single feature can determine the \"real/bogus\" label on its own.\n",
    "1. Use `seaborn` to plot the correlation matrix of this data frame. (See **lecture 15**.)\n",
    "    - Recall that features with correlation 0 are uncorrelated, while features with correlation close to 1 or -1 are highly correlated.\n",
    "    - **Comment:** Look at the \"label\" row. Which features seem to provide the most information about the true label? (In other words, are any features that are especially highly correlated  with \"label\"?)\n",
    "2. Use `seaborn` to make a \"pairplot\" of some of these features. Do not plot all of them, since there are too many to display this way. (See **lecture 19**.)\n",
    "    - Here are ten features you can try: `['diffmaglim', 'magpsf', 'chipsf', 'magdiff', 'classtar', 'aimage', 'bimage', 'elong', 'jdstarthist', 'jdendhist']`\n",
    "    - **Comment:** Which features seem to do the best job at distinguishing \"real\" from \"bogus\" data? Are these the same features you identified in the correlation matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Train a random forest classifier to identify real/bogus images\n",
    "\n",
    "1. Create a [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) object. Then, use the [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) tool to train it on the data in ZTF dataset and test it with 3-fold cross-validation. Determine the optimal parameters for the classifier. (See **homework 9**.)\n",
    "    - Some good features to optimize over are `n_estimators`, `max_features`, `min_samples_leaf`, and `max_depth`.\n",
    "2. Then, divide the data in `ZTF_sources_dataframe` into 75% \"training\" and 25% \"testing\" data. Using the optimal classifier that you identified above, train the model on your training data. Finally, use your trained model to classify the test dataset.\n",
    "3. You should now have **true labels** (from `ZTF_sources_dataframe['label']`) and **predicted labels** (from your random forest classifier) for your test data. What is the **accuracy** of these predictions?\n",
    "    - You should achieve at least a 90% accuracy if your classifier has been tuned correctly. If you've tuned it VERY well, it is possible to achieve 100%!\n",
    "4. Finally, print out the \"feature importances\" for each feature in the random forest classifier (you can use the `RandomForestClassifier` attribute `feature_importances_`). Print out the most important features, **sorted** from highest to lowest importance.\n",
    "    - **Comment:** Do the most important features match those you identified in part 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Unsupervised Machine Learning\n",
    "\n",
    "In this problem, we will now use unsupervised learning to cluster the same data as in Problem 2. We will then run sklearn's [KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) algorithm on 2 or more features.\n",
    "\n",
    "This is a messy dataset that is difficult to cluster. Therefore, this question will walk through a few techniques that can help you evaluate how your clustering is going:\n",
    "1. First, we will visualize the data in 1D and 2D to ensure it is \"well-behaved\" for our algorithm.\n",
    "2. Next, we'll visualize how clustering performs in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from time import time\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib.image import imread\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Plot Features\n",
    "\n",
    "We will perform K-means clustering using two features: 'chipsf' and 'elong'.  Chipsf is the uncertainty associated with performing PSF-fit photometry.  The higher the chi values, the more uncertainty associated with the source's PSF fit. Elong is a measure of how elongated the source is.  A transient point source should have a spherical point spread function.  An elongated point source may be a sign of a problem with image subtraction.\n",
    "\n",
    "Extract features chipsf and elong from the data.  Scatter plot them together, and also plot their histograms.  \n",
    "\n",
    "#### Question: What do you notice about these features?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: KMeans Using Two Features\n",
    "\n",
    "We rarely ever cluster only two features from a dataset.  However, the advantage of doing so is that we can readily visualize two-dimensional data.  Let's start off by clustering features elong and chipsf with KMeans.  The plotKMeans function below implements a visualization of KMean's partitioning that was used in sklearn's [KMean's demo](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html).  \n",
    "\n",
    "Use the runKMeans and plotKMeans functions to cluster the data (feats_selected) with several values of k.  \n",
    "\n",
    "#### Question: What do you think about the quality of the clusters produced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def runKMeans(dat, n_clusters=2, seed=0):\n",
    "        return KMeans(n_clusters, random_state=seed).fit(dat) \n",
    "\n",
    "def plotKMeans(kmeans_res, reduced_dat, xlabel, ylabel, xscale='linear', yscale='linear'):\n",
    "    \n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = reduced_dat[:, 0].min() - 1, reduced_dat[:, 0].max() + 1\n",
    "    y_min, y_max = reduced_dat[:, 1].min() - 1, reduced_dat[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = kmeans_res.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.imshow(Z, interpolation='nearest',\n",
    "               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "               cmap=plt.cm.Paired,\n",
    "               aspect='auto', origin='lower')\n",
    "    plt.plot(reduced_dat[:,0], reduced_dat[:,1], 'k.')\n",
    "    plt.scatter(kmeans_res.cluster_centers_[:, 0], kmeans_res.cluster_centers_[:, 1],\n",
    "                marker='x', s=169, linewidths=3,\n",
    "                color='w', zorder=10)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xscale(xscale)\n",
    "    plt.yscale(yscale)\n",
    "    plt.show()\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Feature Scaling\n",
    "\n",
    "We just discovered that distance metrics can be sensitive to the scale of your data (e.g., some features span large numeric ranges, but others don't).  For machine learning methods that calculate similiarty between feature vectors, it is important to normalize data within a standard range such as (0, 1) or with z-score normalization (scaling to unit mean and variance).  Fortunately, sklearn also makes this quite easy.  Please review sklearn's [preprocessing](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) module options, specifically StandardScaler which corresponds to z-score normalization and MinMaxScaler.  Please implement one. \n",
    "\n",
    "After your data has been scaled, scatter plot your rescaled features, and run KMeans with the transformed data.  Compare the results on the transformed data with those above.\n",
    "\n",
    "Try clustering in log10(chipsf) instead. How does it behave? Visualize the data in both linear and log space, pick one, and explain why it's better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 4: Cluster Evaluation by Visual Inspection\n",
    "\n",
    "It can be tempting to let yourself be guided by metrics alone, and the metrics are useful guideposts that can help determine whether you're moving in the right direction. However, the goal of clustering is to reveal structure in your dataset. Fortunately, because the features were extracted from sources that were extracted from images, we can view the cutouts from each source to visually verify whether our clusters contain homogeneous objects.\n",
    "\n",
    "The display methods below give you an opportunity to display random candidates from each cluster, or the candidates that are closest to the cluster center.\n",
    "\n",
    "What features do you notice present in each cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_stamps(candids, fig_title):\n",
    "    \n",
    "    # display five across\n",
    "    num_per_row = 5\n",
    "    \n",
    "    for i, candid in enumerate(candids):\n",
    "        f_stamp = glob.glob(os.path.join(D_STAMPS, 'candid{}*.png'.format(candid)))[0] # there should only be one file returned!\n",
    "        if (i % num_per_row) == 0:\n",
    "            fig = plt.figure(figsize=(18, 3))\n",
    "            fig.suptitle(fig_title)        \n",
    "\n",
    "        ax = fig.add_subplot(1, num_per_row, i%num_per_row + 1)\n",
    "        ax.set_axis_off()\n",
    "        ax.set_title(candid)\n",
    "        stamp = imread(f_stamp)\n",
    "        imshow(stamp)\n",
    "    return\n",
    "\n",
    "def closest_to_centroid(centroid, cluster_feats, cluster_candids):\n",
    "    \n",
    "    dists = euclidean_distances(cluster_feats, centroid.reshape(1, -1))[:,0]\n",
    "    closest_indices = np.argsort(dists)[:10]\n",
    "    return cluster_candids[closest_indices]\n",
    "\n",
    "def show_cluster_stamps(kmeans_res, displayMode='closest', num_to_display=10):\n",
    "    # spits out a random selection of stamps from each cluster\n",
    "    \n",
    "    \n",
    "    for i in range(kmeans_res.n_clusters):\n",
    "        centroid = kmeans_res.cluster_centers_[i, :]\n",
    "        mask = kmeans_res.labels_ == i\n",
    "        cluster_candids = meta[mask]['candid']\n",
    "        cluster_feats = feats_selected_scaled[mask]\n",
    "        if displayMode == 'near_centroid':\n",
    "            selected_candids = closest_to_centroid(centroid, cluster_feats, cluster_candids)\n",
    "        if displayMode == 'random':\n",
    "            np.random.shuffle(cluster_candids)\n",
    "            selected_candids = cluster_candids[:num_to_display]\n",
    "        display_stamps(selected_candids, 'Cluster {}'.format(i))\n",
    "\n",
    "\n",
    "D_STAMPS = 'data/dsfp_ztf_png_stamps'        \n",
    "show_cluster_stamps(kmeans_k5_scaled, 'near_centroid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Quantitative Cluster Evaluation\n",
    "\n",
    "So far, we've been visually verifying our clusters.  We can use quantitative methods to verify our results. \n",
    "\n",
    "The following is a score that does not require labels:\n",
    "- inertia: \"Sum of squared distances of samples to their closest cluster center.\"\n",
    "- Silhouette coefficient: Measures minimal inertia in ratio to distance to next nearest cluster.  The score is higher are clusters become more compact and well-separated.\n",
    "\n",
    "The following scores do require labels, and are documented [here](http://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation).\n",
    "\n",
    "- ARI, AMI measure the similarity between ground_truth labels and predicted_labels.  ARI measure similarity, and AMI measures in terms of mutual information. Random assignments score close to 0, correct assignments close to 1.\n",
    "- homogeneity: purity of the cluster (did all cluster members have the same label?). Scores in [0,1] where 0 is bad.\n",
    "- completeness: did all labels cluster together in a single cluster? Scores in [0,1] where 0 is bad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 300\n",
    "\n",
    "def bench_k_means(estimator, name, data, labels):\n",
    "    t0 = time()\n",
    "    estimator.fit(data)\n",
    "    print('%-9s\\t%.2fs\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
    "          % (name, (time() - t0), estimator.inertia_,\n",
    "             metrics.homogeneity_score(labels, estimator.labels_),\n",
    "             metrics.completeness_score(labels, estimator.labels_),\n",
    "             metrics.v_measure_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "             metrics.silhouette_score(data, estimator.labels_,\n",
    "                                      metric='euclidean',\n",
    "                                      sample_size=sample_size)))\n",
    "\n",
    "labels = feats[:,-1]\n",
    "print(82 * '_')\n",
    "print('init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette')\n",
    "\n",
    "# INSTRUCTIONS: Use the bench_k_means method to compare your clustering results\n",
    "#\n",
    "bench_k_means(kmeans_k2_scaled, 'K2 scaled', feats_selected_scaled, labels)\n",
    "bench_k_means(kmeans_k5_scaled, 'K5 scaled', feats_selected_scaled, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
